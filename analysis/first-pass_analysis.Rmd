---
title: "First-pass analyses"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  pdf_document:
    toc: yes
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---


Introduction
============

Carry out a first-pass analysis of the data in Petrus I.'s bachelor thesis to
see if it is suitable to be written up as a study and submitted to a journal.

Remarks:

- The data has been pre-processed in a separate script (`data/process_data.Rmd`)
- I will mostly try to replicate the analyses in Petrus's BA thesis with a few
extensions; this means I largely follow the order of how the results are reported
in that thesis


Set up workspace
================

Libraries
---------

```{r setup, include=TRUE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(lme4)
```


Data
----

Matching task:

```{r}
# Matching task
d_match <- read.csv("../data/data_matching-task.csv", fileEncoding = "UTF-8")
head(d_match)
```

Form/category task:

```{r}
# form/category task -- two versions to choose from (uncomment/comment one)

# # full version
# d_fc <- read.csv("../data/data_form-category-task.csv", fileEncoding = "UTF-8")

# cleaned version (see "~/data/process_data.Rmd" for details)
d_fc <- read.csv("../data/data_form-category-task_cleaned.csv", fileEncoding = "UTF-8")

head(d_fc)
```


Participant background data, including L2 English proficiency (LexTale):

```{r}
backgr <- read.csv("../data/participant-background.csv", fileEncoding = "UTF-8")
head(backgr)
```


Inference task:

```{r}
d_infe <- read.csv("../data/data_inference-task.csv", fileEncoding = "UTF-8", 
                   stringsAsFactors = FALSE)
# Want to change "visuospatial" to "spatial" for clarity
d_infe$Type[d_infe$Type == "Visuospatial"] <- "Spatial"
head(d_infe)
```



Matching task
=============

The matching task was used to exclude items in the form/category task for those
participants who could not match it correctly (assuming that their performance
on such an item is uninformative if they don't know the meaning of the word).

*NB*:
This resulted in the "cleaned" dataset for the form/category task (see Data
above).

```{r}
head(d_match)
```

How accurate were subjects on this task?

```{r}
d_match %>% group_by(Subject) %>% summarise(Acc = mean(Score)) %>%
  ggplot(aes(x = Acc)) + geom_histogram() + ylim(0, 11)
```

We see that most were quite accurate.

Another way to see that is to account the number of errors

```{r}
d_match$Error <- ifelse(d_match$Score == 1, 0, 1)
d_match %>% group_by(Subject) %>% 
  summarise(NbErrors = sum(Error)) %>%
  group_by(NbErrors) %>%
  summarise(Freq = n()) %>%
  kable
```


Let's look at it from the point of view of the items:

```{r, message = FALSE}
d_match %>% group_by(TargetWord) %>% summarise(Acc = mean(Score)) %>%
  ggplot(aes(x = Acc)) + geom_histogram()
```

There were some items that elicited a large number of errors, especially "hairspray":

```{r}
d_match %>% group_by(TargetWord) %>% 
  summarise(NbErrors = sum(Error)) %>%
  arrange(- NbErrors) %>%
  print(n = 12)
```





Form- and category tasks
=======================

By-subject visualization
------------------

Summarize each participant's data as a single number, namely their mean
accuracy (`Acc`) in each task and language:

```{r}
d_fc_subj <- d_fc %>% 
  group_by(Subject, Task, Language) %>%
  summarise(Acc = mean(Score))
```

Identify outliers (wrt to Tukey/ggplot2 boxplots):

```{r}
d_fc_subj <- d_fc %>% 
  group_by(Subject, Task, Language) %>%
  summarise(Acc = mean(Score))

# To be able to plot the label names of outliers (SPSS does it by default), we
# adapt the solution proposed in
# https://stackoverflow.com/questions/33524669/labeling-outliers-of-boxplots-in-r
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}
# Outlier contains a subject ID if they are an outlier for a task-language combo
d_fc_subj <- d_fc_subj %>%
  group_by(Task, Language) %>%
  mutate(Outlier = ifelse(is_outlier(Acc), Subject, as.numeric(NA)))
head(d_fc_subj)
```



```{r}

ggplot(d_fc_subj, aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Language)) +
  geom_text(aes(label = Outlier), na.rm = TRUE, hjust = -0.5, size = 4) +
  facet_grid(. ~ Task) +
  ylim(.5, 1) +
  theme_bw()
```




By-item visualization
------------------

Summarize the data by items instead (but otherwise analogously as above):

```{r}
d_fc_item <- d_fc %>% 
  group_by(ItemID, Task, Language) %>%
  summarise(Acc = mean(Score))
head(d_fc_item)
```


```{r}
ggplot(d_fc_item, aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Language)) +
  facet_grid(. ~ Task) +
  ylim(.5, 1) +
  theme_bw()

```


Generalized Linear Mixed Model (GLMM) analysis
---------------------------------

The hypothesis is that there should be an interaction between language and
task, such that participants perform worse in the shape task when carrying
it out in the L2, but their performance should not be worse in the category
task.


```{r}
# Sum coding for factors
# Task
contrasts(d_fc$Task) <- contr.sum(2)
colnames(contrasts(d_fc$Task)) <- "Categ_vs_Shape"
contrasts(d_fc$Task)
# Language
contrasts(d_fc$Language) <- - contr.sum(2)
colnames(contrasts(d_fc$Language)) <- "L2_vs_L1"
contrasts(d_fc$Language)
```


```{r}
# fail to converge...

# fm_fc <- glmer(Score ~ Task * Language +
#                  (Task * Language | Subject) +
#                  (Task * Language | ItemID),
#                data = d_fc,
#                family = "binomial")

# fm_fc <- glmer(Score ~ Task * Language +
#                  (Task * Language | Subject) +
#                  (Task + Language | ItemID),
#                data = d_fc,
#                family = "binomial")

# The only model I can get to converge with any random slope by subjects:
# fm_fc7 <- glmer(Score ~ Task * Language +
#                   (1 + Task | Subject) +
#                   (1 | ItemID),
#                 data = d_fc,
#                 family = "binomial")
# summary(fm_fc7)
```



The role of L2 proficiency
-------------------------


### A quick look at proficiency

We have a few measures of participants' L2 proficiency. The most direct and
reliable one is probably their score on the [LexTale](http://www.lextale.com/)
task. But let's compare them:

```{r}
ggpairs(backgr[, c(2, 4:7)], lower = list(continuous = "smooth"))
```

Nothing strongly stands out, but there are few remarks to make:

- We see that the strongest correlatin is between self-rated L2 proficiency and
the score on the LexTale test. This is what one would expect.
- The scores on the LexTale gave a reasonable spread and roughly conform to a
normal distribution

Let's look at the LexTale scores:

```{r}
ggplot(backgr, aes(x = LexTale)) + geom_histogram(binwidth = .05)
```


### Correlation of proficiency and form/category task in L2


```{r}
# Subset form/category task data in L2 only:
d_fc_l2 <- d_fc_subj %>% select(-Outlier) %>% filter(Language == "L2")
# Add their proficiency scores:
d_fc_l2 <- left_join(d_fc_l2, backgr %>% select(Subject, LexTale))
```


Plot:

```{r}
ggplot(d_fc_l2, aes(x = LexTale, y = Acc, colour = Task)) +
  geom_point() +
  geom_smooth(method = "lm")
```

So this seems to go well with our story:

- L2 proficiency did not really predict their performance on the category task
(which was close to ceiling anyway)
- However, it did correlate with their performance on the *shape* task, as we
expected


Here are the corresponding Pearson correlation tests:

```{r}
by(d_fc_l2, d_fc_l2$Task, function(df) with(df, cor.test(Acc, LexTale)))
```



Inference task
=============

Accuracy
--------

We first look at accuracy:


### By subject


Summarize each participant's data as a single number, namely their mean
accuracy (`Acc`) in each task and language:

```{r}
d_infe_subj <- d_infe %>% 
  group_by(Subject, Language, Type) %>%
  summarise(Acc = mean(Score))
head(d_infe_subj)
```


```{r}
ggplot(d_infe_subj, aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Language)) +
  facet_grid(. ~ Type) +
  ylim(0, 1) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  theme_bw()
```


Let us see if either the valid or invalid inferences were more difficult:

```{r, fig.height = 8}
d_infe %>%
  mutate(Val = ifelse(Valid == 1, "valid", "invalid")) %>%
  group_by(Subject, Language, Type, Val) %>%
  summarise(Acc = mean(Score)) %>%
  ggplot(aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Language)) +
  facet_grid(Type ~ Val) +
  ylim(0, 1) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  theme_bw()
```

There is actually not much evidence that invalid inferences are harder than
valid ones, or the other way around.


### By-item (i.e., by inference pattern)

Let's do an analogous visualization but no grouping by inference pattern
(`InferPattern`) rather than subject:

```{r}
d_infe_ite <- d_infe %>%
  mutate(Val = ifelse(Valid == 1, "valid", "invalid")) %>%
  group_by(InferPattern, Language, Type, Val) %>%
  summarise(Acc = mean(Score))

ggplot(d_infe_ite, aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Language)) +
  facet_grid(. ~ Type) +
  ylim(0, 1) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  theme_bw()
```

Interestingly, there seems to be less variation in the items than in subjects.
So all inference patterns were roughly equally hard.

We can also colour the dots with regard to validity of the items, to see if
we see some evidence now of valid vs invalid items having a different degree
of difficulty:

```{r}
ggplot(d_infe_ite, aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Val)) +
  facet_grid(. ~ Type) +
  ylim(0, 1) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  theme_bw()
```


We can make the comparison even more explicit, as we did above between subjects:

```{r, fig.height = 8}
ggplot(d_infe_ite, aes(x = Language, y = Acc)) +
  geom_boxplot(outlier.shape = 4) +
  geom_jitter(height = 0, alpha = .5, aes(colour = Language)) +
  facet_grid(Type ~ Val) +
  ylim(0, 1) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  theme_bw()
```

Perhaps there is an indication that invalid inferences yielded slightly higher
accuracies. This in turn could be due to participants having a slight bias
towards rejecting the validity of inferences...

**It would probably be a good idea to compute d' (*d-prime*) scores!**


Response times
-------------

### General overview

First we need to get an idea of how response times (RTs) were distributed:
How much dispersion was there within and between participants? Were the
distributions very skewed?

```{r}
# Compute by speaker averages for later plotting and add that to data frame
mean_RT <- d_infe %>% group_by(Subject) %>% 
  summarise(Mean_RT = mean(RT),
            Median_RT = median(RT))
d_infe <- left_join(d_infe, mean_RT)
# Order Subject levels according to Median_RT
d_infe$Subject <- factor(d_infe$Subject, levels = mean_RT$Subject[order(mean_RT$Median_RT)])
```


```{r, fig.height=10}
ggplot(d_infe, aes(x = Language, y = RT)) +
  geom_boxplot() +
  facet_wrap(~ Subject) +
  geom_hline(aes(yintercept = Mean_RT), linetype = "dotted", colour = "red") +
  geom_hline(aes(yintercept = Median_RT), linetype = "dashed", colour = "blue") +
  ggtitle("Raw Response Times (no outliers removed)") +
  theme_bw()
```

This plot is not very useful because some extreme observations compress most of
the data to the bottom of the plots.

Here is a data summary:

```{r}
summary(d_infe$RT)
```

While 75% of the data are RT < `r summary(d_infe$RT)[5]`, extreme observations
go above 20 seconds!

Let us just plot the data again but now cutting out observations above 10:

```{r, fig.height=10}
ggplot(d_infe, aes(x = Language, y = RT)) +
  geom_boxplot() +
  facet_wrap(~ Subject) +
  geom_hline(aes(yintercept = Mean_RT), linetype = "dotted", colour = "red") +
  geom_hline(aes(yintercept = Median_RT), linetype = "dashed", colour = "blue") +
  ylim(-0.5, 10) +
  ggtitle("Raw Response Times: only RTs < 10 s") +
  theme_bw()
```

Now we get a much better sense of the data. Some notes about the plot here above:

- The data for each subject is plotted in a separate panel, with L1 data plotted
on the left and L2 data on the right
- Subjects are ordered from fastest to slowest based on their *median* response
time
- Each subject's median is plotted as a blue dashed line; each subject's mean as
a red dotted line. (These lines are close for subjects with RTs roughly normally
distributed)
- Note that for the slowest participants the plots are slightly misleading because
the whiskers are cut off (e.g., participant 14, compare the plot directly here
above with the first that was presented)


### Correct vs incorrect trials

In the previous section, we looked at all the trials, independent of whether
participants' responses were correct or not. However, Knauff and May (2006)
removed incorrect trials from the RT analyses, and so did Petrus. This is
standard practice and it actually makes sense because one may wonder if subjects
were even paying attention on incorrect trials.

First, let's see RTs as a function of response accuracy:

```{r}
ggplot(d_infe, aes(x = factor(Score), y = RT)) +
  geom_boxplot() +
  facet_grid(. ~ Language)
```

Indeed, in both L1 and L2 there is a tendency for incorrect trials to be
associated to longer RTs:

```{r}
d_infe %>% group_by(Language, Score) %>% 
  summarise(Mean_RT = mean(RT), SD_RT = sd(RT)) %>%
  kable
```

Yes, the mean effect is indeed quite substantial: incorrect responses yield
roughly 1.5 seconds longer RTs.


### Data loss when removing incorrect trials

How many observations will be removed?

```{r}
table(d_infe$Score)
```

This amounts to `r round(100 * table(d_infe$Score)[1] / nrow(d_infe), 1)`%
of the data.

Of interest is to see how much that will affect particular participants and
particular items:

#### Participants:

```{r}
d_infe %>%
  mutate(Error = ifelse(Score == 0, 1, 0)) %>%
  group_by(Subject) %>%
  summarise(nbErrors = sum(Error), nbObs = n()) %>%
  mutate(dataLossPerc = round(100 * nbErrors / nbObs)) %>%
  arrange(-nbErrors) %>%
  select(Subject, nbErrors, dataLossPerc) %>%
  print(n = 24)
```

For some participants (e.g., 23 or 22) data loss is quite substantial.

We can actually break it down by Language to see if some cases become even more
extreme then:

```{r}
d_infe %>%
  mutate(Error = ifelse(Score == 0, 1, 0)) %>%
  group_by(Subject, Language) %>%
  summarise(nbErrors = sum(Error), nbObs = n()) %>%
  mutate(dataLossPerc = round(100 * nbErrors / nbObs)) %>%
  arrange(-nbErrors) %>%
  select(Subject, Language, nbErrors, dataLossPerc)
```

The picture does not change radically.

In any case, it makes sense to exclude incorrect trials from further analysis:

```{r}
d_infe_corr <- d_infe %>% filter(Score == 1)
```


#### Items (inference patterns):

```{r}
d_infe %>%
  mutate(Error = ifelse(Score == 0, 1, 0)) %>%
  group_by(InferPattern, Type) %>%
  summarise(nbErrors = sum(Error), nbObs = n()) %>%
  mutate(dataLossPerc = round(100 * nbErrors / nbObs)) %>%
  arrange(-nbErrors) %>%
  select(InferPattern, nbErrors, dataLossPerc)
```

The differences in accuracy between items / inference patterns are not as
pronounced as between subjects.

### Exclude incorrect trials

In any case, it makes sense to exclude incorrect trials from further analysis:

```{r}
d_infe_corr <- d_infe %>% filter(Score == 1)
```





### Log-transorm the data to avoid the strong right-skew?

If we look at the raw RTs data as a whole, we see that it is strongly
right-skewed:

```{r}
# Following
# http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/
ggplot(d_infe_corr, aes(x = RT)) + 
  geom_histogram(aes(y=..density..),  # Histogram with density instead of count on y-axis
                 binwidth=.5, colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666")  # Overlay with transparent density plot
```

In situations like this, taking the log of the DV can be helpful:


```{r}
# Because log of negative number is not defined, we somewhat arbitrarily add
# 0.5 to all RTs (just a shift in scale) to make them all positive:
d_infe_corr$logRT <- log(d_infe_corr$RT + 0.5)
ggplot(d_infe_corr, aes(x = logRT)) + 
  geom_histogram(aes(y=..density..),  # Histogram with density instead of count on y-axis
                 colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666")  # Overlay with transparent density plot
```


The distribution of `logRT`s is much less skewed, although we might now
observe a slight left-skew instead. Note that this transformation is a
monotonically increasing function, so the order of the observations is
preserved.

Let us look at the same plot of by-subject observations as above, but
now using the log-transformed RT data.

```{r}
# Compute by speaker averages and medians for logRT
mean_logRT <- d_infe_corr %>% group_by(Subject) %>% 
  summarise(Mean_logRT = mean(logRT),
            Median_logRT = median(logRT))
d_infe_corr <- left_join(d_infe_corr, mean_logRT)
# Order Subject levels according to Median_logRT now
d_infe_corr$Subject <- factor(
  d_infe_corr$Subject, 
  levels = mean_logRT$Subject[order(mean_logRT$Median_logRT)]
  )
```


```{r, fig.height=10}
ggplot(d_infe_corr, aes(x = Language, y = logRT)) +
  geom_boxplot() +
  facet_wrap(~ Subject) +
  geom_hline(aes(yintercept = Mean_logRT), linetype = "dotted", colour = "red") +
  geom_hline(aes(yintercept = Median_logRT), linetype = "dashed", colour = "blue") +
  ggtitle("Log-transformed Response Times (no observations removed)") +
theme_bw()
```

Under the log transformation, the problem of removing outliers, which always
involves arbitrary cutoffs, is largely avoided.

**We will therefore proceed with log-transformed data for now. Later, however, we will re-run the analyses using the approach adopted by Knauff & May (2006)**,
which is also what Petrus did in his follow-up analysis.

A note on the interpretation of log-RTs in regression analysis:

There is a different assumption underlying the choice of whether to analyze the
dependent variable reaction time as raw scores or on a logarithmic scale.
The assumption has to do with how one believes the manipulations to be related
to the DV. Taking raw RTs assumes that the effect of the predictor on the DV will
be linear, i.e. a regression coefficient of 2 would indicate that RTs increase
by adding 2 for any unit you move on the predictor. However, if you model the
DV as logRT, we assume that the effect is multiplicative. This means that a
regression coefficient of 2 indicates that RTs are *multiplied* by 2.

This is important to keep in mind for the analyses.


### By subject


```{r}
# Summarize RTs by subject
rt_subj <- d_infe_corr %>%
  group_by(Subject, Language, Type) %>%
  summarise(logRT = mean(logRT))
head(rt_subj, 7)
```


```{r}
ggplot(rt_subj, aes(x = Language, y = logRT, colour = Language)) +
  geom_boxplot() +
  facet_grid(. ~ Type)
```

Indeed, the data at least numerically show the expected tendency:
The visual type of inferences were more demanding in the L1 than in the L2.

Perhaps a more insightful plot is one that is created by plotting per-cell
means and 95% confidence intervals (assuming normality, i.e. +/- 2 SE).

```{r}
sem_fun <- function(x) sd(x) / sqrt(length(x))
rt_subj_summ <- rt_subj %>%
  group_by(Language, Type) %>%
  summarise(M = mean(logRT), SE = sem_fun(logRT))
rt_subj_summ
```

First, just means and 95% confidence intervals:

```{r}
ggplot(rt_subj_summ, aes(x = Language, y = M, colour = Language)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  facet_grid(. ~ Type) +
  theme_bw()
```

But we can also overlay the individual subject means to get a better idea of
the dispersion in the data.

```{r}
ggplot(rt_subj_summ, aes(x = Language, y = M)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  geom_jitter(data = rt_subj, aes(y = logRT, colour = Language), height = 0, 
              alpha = .4) +
  facet_grid(. ~ Type) +
  theme_bw()
```


We can add a little bit more of information to the graph by connecting
observations from the same speaker in the different languages. This basically
shows individual by-subject slopes (as will later be captured by the random
effects in linear mixed model, LMM).

```{r}
ggplot(rt_subj_summ, aes(x = Language, y = M)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  geom_jitter(data = rt_subj, aes(y = logRT, colour = Language), height = 0, 
              alpha = .4) +
  geom_line(data = rt_subj, aes(y = logRT, group = Subject), alpha = .3) +
  facet_grid(. ~ Type) +
  theme_bw()
```


For AMLaP abstract:

```{r}
# NB: We remove control condition, for clarity.
rt_subj_amlap <- rt_subj %>% filter(Type != "Control") 
rt_subj_amlap
rt_subj_summ <- rt_subj %>%
  group_by(Language, Type) %>%
  summarise(M = mean(logRT), SE = sem_fun(logRT))

rt_subj_summ %>%
  filter(Type != "Control") %>%
ggplot(aes(x = Language, y = M)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  geom_jitter(data = rt_subj %>% filter(Type != "Control"), aes(y = logRT, colour = Language), height = 0, 
              alpha = .4) +
  facet_grid(. ~ Type) +
  ylab("Response Times\n(log seconds)") +
  theme_bw()
```

Save to disk (perhaps uncomment next line?)

```{r}
ggsave("fig_exp2_amlap.png", height = 2.5, width = 5)
```



### LMM analysis

Appropriate coding for factors:


```{r}
# Sum coding for Language
d_infe_corr$Language <- factor(d_infe_corr$Language)
contrasts(d_infe_corr$Language) <- - contr.sum(2)
colnames(contrasts(d_infe_corr$Language)) <- "L2_vs_L1"
contrasts(d_infe_corr$Language)

# Type, with 2 possibilities
# 1) Helmert coding
d_infe_corr$Type <- factor(d_infe_corr$Type)
contrasts(d_infe_corr$Type) <- contr.helmert(3)
colnames(contrasts(d_infe_corr$Type)) <- c("Spat_vs_Cont", "Visual_vs_rest")
contrasts(d_infe_corr$Type)
contrasts(d_infe_corr$Language)
# # 2) Treatment/dummy coding
# d_infe_corr$Type <- factor(d_infe_corr$Type, 
#                            levels = c( "Visual", "Control", "Visuospatial"))
# contrasts(d_infe_corr$Type) <- contr.treatment(3)
# colnames(contrasts(d_infe_corr$Type)) <- c("Contr-vs-Visu", "Spat-vs-Visu")
# contrasts(d_infe_corr$Type)
```


Run the models (or comment them as they take a long time)

```{r}
# # Model with critical interaction
# fm_RT <- lmer(logRT ~ Language * Type +
#                 (1 + Language * Type | Subject) +
#                 (1 + Language * Type | InferPattern),
#               data = d_infe_corr, REML = FALSE)
# summary(fm_RT)
# # Model WITHOUT critical interaction
# fm_RT_nointeract <- lmer(logRT ~ Language + Type +
#                            (1 + Language * Type | Subject) +
#                            (1 + Language * Type | InferPattern),
#                          data = d_infe_corr, REML = FALSE)
# summary(fm_RT_nointeract)
```

The interaction of Type and Language does not come out as significant in a
LRT:

```{r}
# anova(fm_RT, fm_RT_nointeract)
```

```
> anova(fm_RT, fm_RT_nointeract)
Data: d_infe_corr
Models:
fm_RT_nointeract: logRT ~ Language + Type + (1 + Language * Type | Subject) + (1 + 
fm_RT_nointeract:     Language * Type | InferPattern)
fm_RT: logRT ~ Language * Type + (1 + Language * Type | Subject) + (1 + 
fm_RT:     Language * Type | InferPattern)
                 Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)
fm_RT_nointeract 47 1521.7 1751.1 -713.82   1427.7                         
fm_RT            49 1523.1 1762.3 -712.54   1425.1 2.5616      2     0.2778
```

Here's the model output for the model with the interaction:

```
> summary(fm_RT)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: logRT ~ Language * Type + (1 + Language * Type | Subject) + (1 +      Language * Type | InferPattern)
   Data: d_infe_corr

     AIC      BIC   logLik deviance df.resid 
  1523.1   1762.3   -712.5   1425.1      925 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5047 -0.6103 -0.1109  0.4601  4.3374 

Random effects:
 Groups       Name                                Variance  Std.Dev. Corr                         
 Subject      (Intercept)                         0.1381110 0.37163                               
              LanguageL2_vs_L1                    0.0171943 0.13113  -0.48                        
              TypeSpat_vs_Cont                    0.0050948 0.07138   0.12 -0.21                  
              TypeVisual_vs_rest                  0.0016607 0.04075  -0.34 -0.35  0.23            
              LanguageL2_vs_L1:TypeSpat_vs_Cont   0.0041746 0.06461   0.55 -0.37 -0.70 -0.39      
              LanguageL2_vs_L1:TypeVisual_vs_rest 0.0009023 0.03004   0.41 -0.78  0.17  0.69  0.19
 InferPattern (Intercept)                         0.0007861 0.02804                               
              LanguageL2_vs_L1                    0.0026117 0.05111   0.50                        
              TypeSpat_vs_Cont                    0.0023259 0.04823  -0.66  0.13                  
              TypeVisual_vs_rest                  0.0002836 0.01684  -0.58  0.38  0.93            
              LanguageL2_vs_L1:TypeSpat_vs_Cont   0.0011324 0.03365   0.47 -0.44 -0.94 -0.98      
              LanguageL2_vs_L1:TypeVisual_vs_rest 0.0006039 0.02458   0.56 -0.07 -0.19 -0.45  0.27
 Residual                                         0.2157622 0.46450                               
Number of obs: 974, groups:  Subject, 24; InferPattern, 8

Fixed effects:
                                      Estimate Std. Error t value
(Intercept)                          0.8523357  0.0779858  10.929
LanguageL2_vs_L1                    -0.0650490  0.0356519  -1.825
TypeSpat_vs_Cont                    -0.0443093  0.0290458  -1.525
TypeVisual_vs_rest                   0.0002383  0.0147207   0.016
LanguageL2_vs_L1:TypeSpat_vs_Cont    0.0193417  0.0256148   0.755
LanguageL2_vs_L1:TypeVisual_vs_rest  0.0222186  0.0149935   1.482

Correlation of Fixed Effects:
            (Intr) LnL2__L1 TyS__C TypV__ LL2__L1:TS
LnggL2_v_L1 -0.323                                  
TypSpt_vs_C  0.009 -0.044                           
TypVsl_vs_r -0.219 -0.066    0.288                  
LL2__L1:TS_  0.304 -0.246   -0.455 -0.296           
LL2__L1:TV_  0.208 -0.262   -0.029  0.052  0.113    
convergence code: 0
singular fit
```

### A somewhat indirect subject analysis

One could also see if there is a correlation between how proficient speakers
are in their L2 and how much of a "penalty" they pay for doing the inferences
of the visual type.

The idea would here be that the lower your L2 proficiency is, the bigger
the difference should be between how fast you are answering these inferences
in your L1 and L2, and crucially, the relation should be as follows:

**Prediction**:

With lower L2 proficiency, the difference between RTs in the L1 and L2 for
visual inferences should be greater (with an advantage in L2); with higher
proficiency, the difference in RTs should be reduced or even zero.

Let us compute, for each subject, the difference in their L1/L2 RTs for visual
items:

```{r}
RTs_proficiency <- d_infe_corr %>%
  filter(Type == "Visual") %>%
  group_by(Subject, Language) %>%
  summarise(M = mean(logRT)) %>%
  # From long to wide
  spread(Language, M) %>%
  # compute difference
  mutate(L1_minus_L2 = L1 - L2) %>%
  # Join with information about proficiency and accuracy in the shape task
  left_join(d_fc_l2 %>%
              ungroup() %>%
              mutate(Subject = as.character(Subject)) %>% 
              filter(Task == "Shape") %>% 
              select(Subject, Acc, LexTale))
head(RTs_proficiency)
```

```{r}
ggplot(RTs_proficiency, aes(x = LexTale, y = L1_minus_L2)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Pearson correlation test

```{r}
with(RTs_proficiency, cor.test(L1_minus_L2, LexTale))
```


What about the accuracy?

```{r}
ggplot(RTs_proficiency, aes(x = Acc, y = L1_minus_L2)) +
  geom_point() +
  geom_smooth(method = "lm")
```


Nahh...


```{r}
with(RTs_proficiency, cor.test(L1_minus_L2, Acc))
```



### Analysis replacing outliers (as done by Knauff & May, 2006)


Knauff and May (2006) do not log-transform RTs. Instead they replace outliers
with a certain cutoff value. Petrus in his thesis adopts the same approach
in his follow-up analysis. This is described on p. 15-16 of his thesis (see
`190107_isaksson_uppsats_slutv.pdf`).

Here is how the procedure is described by Petrus:

> Ett medelvärde räknades alltså ut för varje betingelse (här språk-typ) och
de responstider som låg utanför avskärningspunkten för betingelsen -– "the 
cut-off of the condition” –- ersattes med detta värde. [...] Värdet för
avskärningspunkten refererar alltså här till (2(SD) + medel) för respektive
betingelse.

I follow this approach and then carry out the same visualizations as above.
I try to be explicit at each step so that it becomes easy to discover if
I have done something differently to Petrus.


#### Trimming the observations

```{r}
# Compute mean and SD of all RTs in each of the cells given by the six
# Language-Type combinations:
RT_cutoff <- d_infe_corr %>%
  group_by(Language, Type) %>%
  summarise(M = mean(RT), SD = sd(RT))
head(RT_cutoff)
# Based on this we compute the upper and lower cutoff points:
RT_cutoff <- RT_cutoff %>%
  mutate(lower_cutoff = M - 2 * SD, upper_cutoff = M + 2 * SD)
head(RT_cutoff)
```

First we can see that no observations falls *below* any of the lower cutoff points,
since the lowers RT is `r round(min(d_infe_corr$RT), 2)`.
So we need only to worry about observations higher than the upper cutoff points.

```{r}
# add the upper cutoff points to the data set:
d_infe_corr <- left_join(d_infe_corr, 
                         RT_cutoff %>% select(Language, Type, upper_cutoff))
head(d_infe_corr)
# Add column containing RTs or cutoff points for outliers
d_infe_corr <- d_infe_corr %>%
  mutate(
    # Replaces outliers with cutoffs
    RT_trimmed = ifelse(RT <= upper_cutoff, RT, upper_cutoff),
    # Indicator variable to keep track of trimmed responses
    Trimmed = ifelse(RT <= upper_cutoff, 0, 1)
    )
head(d_infe_corr)
```

We should of course also ask ourselves how many observations we are trimming
in this way:

```{r}
d_infe_corr %>%
  group_by(Language, Type) %>%
  summarise(NbTrimmed = sum(Trimmed))
```

Not that many: `r sum(d_infe_corr$Trimmed)` trimmed observations in total.

Finally, let's look at the distribution of data after trimming. Of course,
the effect has been to cut off the long right tail of the distribution, while
at the same time inflating the density for observations close to the cutoff
(i.e., around 7 seconds):

```{r}
ggplot(d_infe_corr, aes(x = RT_trimmed)) + 
  geom_histogram(aes(y=..density..),  # Histogram with density instead of count on y-axis
                 binwidth=.5, colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666")  # Overlay with transparent density plot
```


#### Repeat analyses above


##### By subject


```{r}
# Summarize RTs by subject
rt_subj <- d_infe_corr %>%
  group_by(Subject, Language, Type) %>%
  summarise(RT_trimmed = mean(RT_trimmed))
head(rt_subj, 7)
```


```{r}
ggplot(rt_subj, aes(x = Language, y = RT_trimmed, colour = Language)) +
  geom_boxplot() +
  facet_grid(. ~ Type)
```

Indeed, the data at least numerically show the expected tendency:
The visual type of inferences were more demanding in the L1 than in the L2.

Perhaps a more insightful plot is one that is created by plotting per-cell
means and 95% confidence intervals (assuming normality, i.e. +/- 2 SE).

```{r}
rt_subj_summ <- rt_subj %>%
  group_by(Language, Type) %>%
  summarise(M = mean(RT_trimmed), SE = sem_fun(RT_trimmed))
rt_subj_summ
```

First, just means and 95% confidence intervals:

```{r}
ggplot(rt_subj_summ, aes(x = Language, y = M, colour = Language)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  facet_grid(. ~ Type) +
  theme_bw()
```

But we can also overlay the individual subject means to get a better idea of
the dispersion in the data.

```{r}
ggplot(rt_subj_summ, aes(x = Language, y = M)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  geom_jitter(data = rt_subj, aes(y = RT_trimmed, colour = Language), height = 0, 
              alpha = .4) +
  facet_grid(. ~ Type) +
  theme_bw()
```


We can add a little bit more of information to the graph by connecting
observations from the same speaker in the different languages. This basically
shows individual by-subject slopes (as will later be captured by the random
effects in linear mixed model, LMM).

```{r}
ggplot(rt_subj_summ, aes(x = Language, y = M)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = M - 1.96 * SE, ymax = M + 1.96 * SE), width = .25,
                size = 1) +
  geom_jitter(data = rt_subj, aes(y = RT_trimmed, colour = Language), height = 0, 
              alpha = .4) +
  geom_line(data = rt_subj, aes(y = RT_trimmed, group = Subject), alpha = .3) +
  facet_grid(. ~ Type) +
  theme_bw()
```





##### LMM analysis

Appropriate coding for factors: see section
*Inference Task > Response times > LMM analysis*
above.

Run the models (or comment them as they take a long time)

```{r}
# # Model with critical interaction
# fm_RT_trimmed <- lmer(RT_trimmed ~ Language * Type +
#                 (1 + Language * Type | Subject) +
#                 (1 + Language * Type | InferPattern),
#               data = d_infe_corr, REML = FALSE)
# summary(fm_RT_trimmed)
# # Model WITHOUT critical interaction
# fm_RT_trimmed_nointeract <- lmer(RT_trimmed ~ Language + Type +
#                            (1 + Language * Type | Subject) +
#                            (1 + Language * Type | InferPattern),
#                          data = d_infe_corr, REML = FALSE)
# summary(fm_RT_trimmed_nointeract)
```

The interaction of Type and Language does not come out as significant in a
LRT:

```{r}
# anova(fm_RT_trimmed, fm_RT_trimmed_nointeract)
```

```
> anova(fm_RT_trimmed, fm_RT_trimmed_nointeract)
Data: d_infe_corr
Models:
fm_RT_trimmed_nointeract: RT_trimmed ~ Language + Type + (1 + Language * Type | Subject) + 
fm_RT_trimmed_nointeract:     (1 + Language * Type | InferPattern)
fm_RT_trimmed: RT_trimmed ~ Language * Type + (1 + Language * Type | Subject) + 
fm_RT_trimmed:     (1 + Language * Type | InferPattern)
                         Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)
fm_RT_trimmed_nointeract 47 3491.6 3721.0 -1698.8   3397.6                         
fm_RT_trimmed            49 3491.9 3731.1 -1697.0   3393.9 3.6677      2     0.1598
```

Here's the model output for the model with the interaction:

```
> summary(fm_RT_trimmed)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT_trimmed ~ Language * Type + (1 + Language * Type | Subject) +      (1 + Language * Type | InferPattern)
   Data: d_infe_corr

     AIC      BIC   logLik deviance df.resid 
  3491.9   3731.1  -1697.0   3393.9      925 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.3635 -0.6018 -0.2069  0.3240  3.5815 

Random effects:
 Groups       Name                                Variance Std.Dev. Corr                         
 Subject      (Intercept)                         0.735373 0.85754                               
              LanguageL2_vs_L1                    0.071684 0.26774  -0.51                        
              TypeSpat_vs_Cont                    0.060490 0.24595   0.13  0.16                  
              TypeVisual_vs_rest                  0.013906 0.11793  -0.36 -0.43 -0.61            
              LanguageL2_vs_L1:TypeSpat_vs_Cont   0.031803 0.17833  -0.57  0.27 -0.86  0.51      
              LanguageL2_vs_L1:TypeVisual_vs_rest 0.009635 0.09816   0.51 -0.90 -0.37  0.60 -0.09
 InferPattern (Intercept)                         0.010864 0.10423                               
              LanguageL2_vs_L1                    0.014260 0.11942   0.05                        
              TypeSpat_vs_Cont                    0.014847 0.12185   0.93 -0.32                  
              TypeVisual_vs_rest                  0.003268 0.05717  -0.89  0.40 -1.00            
              LanguageL2_vs_L1:TypeSpat_vs_Cont   0.009561 0.09778  -0.93  0.31 -0.99  0.99      
              LanguageL2_vs_L1:TypeVisual_vs_rest 0.001235 0.03515   0.50  0.79  0.19 -0.11 -0.24
 Residual                                         1.661521 1.28900                               
Number of obs: 974, groups:  Subject, 24; InferPattern, 8

Fixed effects:
                                    Estimate Std. Error t value
(Intercept)                          2.24023    0.18373  12.193
LanguageL2_vs_L1                    -0.20600    0.08072  -2.552
TypeSpat_vs_Cont                     0.13564    0.08366   1.621
TypeVisual_vs_rest                  -0.04467    0.04300  -1.039
LanguageL2_vs_L1:TypeSpat_vs_Cont   -0.07595    0.07168  -1.060
LanguageL2_vs_L1:TypeVisual_vs_rest  0.07021    0.03761   1.867

Correlation of Fixed Effects:
            (Intr) LnL2__L1 TyS__C TypV__ LL2__L1:TS
LnggL2_v_L1 -0.329                                  
TypSpt_vs_C  0.168 -0.018                           
TypVsl_vs_r -0.276 -0.058   -0.446                  
LL2__L1:TS_ -0.367  0.167   -0.525  0.369           
LL2__L1:TV_  0.296 -0.194   -0.086  0.159 -0.064    
convergence code: 0
singular fit
```

While this way of analyzing the data results in a somewhat stronger coefficient
estimate for the critical interaction, it still does not come out as statistically
significant.
